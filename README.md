# Mini-GPT

![Python](https://img.shields.io/badge/language-Python-blue.svg) ![Repo Size](https://img.shields.io/github/repo-size/PawanKiitb/mini-gpt) ![Last Commit](https://img.shields.io/github/last-commit/PawanKiitb/mini-gpt)

Mini-GPT is a lightweight implementation of a Generative Pre-trained Transformer (GPT) model. It aims to provide a simplified yet effective way to understand and experiment with GPT architectures for natural language processing tasks.

## Features

- **Lightweight**: Implements the core concepts of GPT without unnecessary complexity.
- **Educational**: Ideal for learning and experimenting with GPT architectures.

## ðŸ“‚ Project Structure

```
mini-gpt/
â”œâ”€â”€ bigram.py      # Bigram Model
â”œâ”€â”€ gpt.py         # mini-GPT Model
â”œâ”€â”€ input.txt      # Training scripts
â””â”€â”€ README.md      # Project documentation
```

## ðŸ“– Learn More

- [My Report on LLMs](https://drive.google.com/file/d/1q92vU-WlpmJsxlZiRjfwSr1Pvgpmbk8Z/view?usp=sharing)
- [Transformer Architecture](https://arxiv.org/abs/1706.03762)
